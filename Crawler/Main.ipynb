{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea19c97f-eb3b-4cf1-a58a-7758bd6fd412",
   "metadata": {},
   "source": [
    "\"웹 크롤러\"\n",
    "뉴스 데이터 크롤링 및 경제/금융 문제 & 해설 데이터셋 수집 by. 권수현 / 차시현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a841fd5-b9ff-4368-9a91-01225adcf8e8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sashi\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: fake-useragent in c:\\users\\sashi\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\sashi\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\sashi\\anaconda3\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\sashi\\anaconda3\\lib\\site-packages (5.2.1)\n",
      "Collecting XlsxWriter\n",
      "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sashi\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\sashi\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sashi\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sashi\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sashi\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\sashi\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sashi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
      "Installing collected packages: XlsxWriter\n",
      "Successfully installed XlsxWriter-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4 fake-useragent pandas openpyxl lxml XlsxWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8069693f-f7d9-4757-8f1b-d00f15d47e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "82c59031-26d5-48d2-b6e5-64a301b980cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_naver_news_url(query, start_date, end_date, sort=0):\n",
    "    base_url = \"https://search.naver.com/search.naver\"\n",
    "    query_param = f\"query={query.replace(' ', '+')}\"\n",
    "    ds_param = f\"ds={start_date}\"\n",
    "    de_param = f\"de={end_date}\"\n",
    "    sort_param = f\"sort={sort}\"\n",
    "\n",
    "    url = f\"{base_url}?where=news&{query_param}&{ds_param}&{de_param}&{sort_param}&start=1\"\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6501fd56-fc7c-40ef-85e5-024f592209d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_news(query, start_date, end_date, page_limit=21):\n",
    "    news_data = []\n",
    "\n",
    "    for page in range(page_limit):\n",
    "        start = page * 10 + 1\n",
    "        search_url = generate_naver_news_url(query, start_date, end_date)[:-1] + str(start)\n",
    "\n",
    "        print(f\"크롤링 중: {search_url}\")\n",
    "        resp = requests.get(search_url)\n",
    "        html = resp.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        totalNews = soup.select('div.news_area')\n",
    "\n",
    "        for m in totalNews:\n",
    "            title = m.select_one('a.news_tit').text\n",
    "            url = m.select_one('a.news_tit')['href']\n",
    "            message = m.select_one('div.dsc_wrap > a.api_txt_lines.dsc_txt_wrap').text\n",
    "            press = m.select_one('a.info.press').text\n",
    "\n",
    "            news_data.append({\n",
    "                'title': title,\n",
    "                'url': url,\n",
    "                'message': message,\n",
    "                'press': press\n",
    "            })\n",
    "\n",
    "    return news_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2650aba0-3765-4cd1-bd8d-dfa2066463a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색할 쿼리를 입력하세요:  경제\n",
      "시작 날짜를 입력하세요 (YYYY.MM.DD 형식):  2012.01.01\n",
      "종료 날짜를 입력하세요 (YYYY.MM.DD 형식):  2024.10.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크롤링 중: https://search.naver.com/search.naver?where=news&query=경제&ds=2012.01.01&de=2024.10.39&sort=0&start=1\n",
      "크롤링 중: https://search.naver.com/search.naver?where=news&query=경제&ds=2012.01.01&de=2024.10.39&sort=0&start=11\n",
      "크롤링 중: https://search.naver.com/search.naver?where=news&query=경제&ds=2012.01.01&de=2024.10.39&sort=0&start=21\n",
      "크롤링 중: https://search.naver.com/search.naver?where=news&query=경제&ds=2012.01.01&de=2024.10.39&sort=0&start=31\n",
      "크롤링 중: https://search.naver.com/search.naver?where=news&query=경제&ds=2012.01.01&de=2024.10.39&sort=0&start=41\n",
      "크롤링 중: https://search.naver.com/search.naver?where=news&query=경제&ds=2012.01.01&de=2024.10.39&sort=0&start=51\n",
      "크롤링 중: https://search.naver.com/search.naver?where=news&query=경제&ds=2012.01.01&de=2024.10.39&sort=0&start=61\n",
      "크롤링 중: https://search.naver.com/search.naver?where=news&query=경제&ds=2012.01.01&de=2024.10.39&sort=0&start=71\n",
      "크롤링 중: https://search.naver.com/search.naver?where=news&query=경제&ds=2012.01.01&de=2024.10.39&sort=0&start=81\n",
      "크롤링 중: https://search.naver.com/search.naver?where=news&query=경제&ds=2012.01.01&de=2024.10.39&sort=0&start=91\n",
      "크롤링 중: https://search.naver.com/search.naver?where=news&query=경제&ds=2012.01.01&de=2024.10.39&sort=0&start=101\n",
      "크롤링 중: https://search.naver.com/search.naver?where=news&query=경제&ds=2012.01.01&de=2024.10.39&sort=0&start=111\n",
      "크롤링 중: https://search.naver.com/search.naver?where=news&query=경제&ds=2012.01.01&de=2024.10.39&sort=0&start=121\n",
      "크롤링 중: https://search.naver.com/search.naver?where=news&query=경제&ds=2012.01.01&de=2024.10.39&sort=0&start=131\n",
      "크롤링 중: https://search.naver.com/search.naver?where=news&query=경제&ds=2012.01.01&de=2024.10.39&sort=0&start=141\n",
      "크롤링 중: https://search.naver.com/search.naver?where=news&query=경제&ds=2012.01.01&de=2024.10.39&sort=0&start=151\n",
      "크롤링 중: https://search.naver.com/search.naver?where=news&query=경제&ds=2012.01.01&de=2024.10.39&sort=0&start=161\n",
      "크롤링 중: https://search.naver.com/search.naver?where=news&query=경제&ds=2012.01.01&de=2024.10.39&sort=0&start=171\n",
      "크롤링 중: https://search.naver.com/search.naver?where=news&query=경제&ds=2012.01.01&de=2024.10.39&sort=0&start=181\n",
      "크롤링 중: https://search.naver.com/search.naver?where=news&query=경제&ds=2012.01.01&de=2024.10.39&sort=0&start=191\n",
      "크롤링 중: https://search.naver.com/search.naver?where=news&query=경제&ds=2012.01.01&de=2024.10.39&sort=0&start=201\n",
      "크롤링 완료 및 저장: news_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    query = input(\"검색할 쿼리를 입력하세요: \")\n",
    "    start_date = input(\"시작 날짜를 입력하세요 (YYYY.MM.DD 형식): \")\n",
    "    end_date = input(\"종료 날짜를 입력하세요 (YYYY.MM.DD 형식): \")\n",
    "    \n",
    "    news_results = crawl_news(query, start_date, end_date)\n",
    "    \n",
    "    # 결과를 DataFrame으로 변환하고 Excel로 저장\n",
    "    df = pd.DataFrame(news_results)\n",
    "    df.to_excel(\"news_results.xlsx\", index=False)\n",
    "    print(\"크롤링 완료 및 저장: news_results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a8798-d483-4e0c-b1cb-6d073adbf223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def load_news_data(file_path):\n",
    "    \"\"\"Excel 파일에서 뉴스 데이터를 로드합니다.\"\"\"\n",
    "    return pd.read_excel(file_path)\n",
    "\n",
    "def load_dictionary(file_path):\n",
    "    \"\"\"JSON 파일에서 경제 용어 데이터를 로드합니다.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extract_keywords(news_data, dictionary):\n",
    "    \"\"\"뉴스 데이터에서 키워드를 추출합니다.\"\"\"\n",
    "    keywords = set()  # 중복 방지를 위해 set 사용\n",
    "\n",
    "    for _, row in news_data.iterrows():\n",
    "        title = row['title']\n",
    "        message = row['message']\n",
    "        \n",
    "        # 뉴스 제목과 메시지에서 각 경제 용어가 포함되어 있는지 확인\n",
    "        for term in dictionary.keys():\n",
    "            if term in title or term in message:\n",
    "                keywords.add(term)\n",
    "\n",
    "    return list(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c7f2af-14fc-43bb-8885-0b6cdc96f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_keywords(keywords, file_path):\n",
    "    \"\"\"키워드를 JSON 파일로 저장합니다.\"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(keywords, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 데이터 경로 설정\n",
    "    news_file_path = \"news_results.xlsx\"\n",
    "    dict_file_path = \"dictdb.json\"\n",
    "    keywords_file_path = \"keywords.json\"\n",
    "\n",
    "    # 데이터 로드\n",
    "    news_data = load_news_data(news_file_path)\n",
    "    dictionary = load_dictionary(dict_file_path)\n",
    "\n",
    "    # 키워드 추출\n",
    "    keywords = extract_keywords(news_data, dictionary)\n",
    "\n",
    "    # 결과 저장\n",
    "    save_keywords(keywords, keywords_file_path)\n",
    "    print(f\"추출된 키워드를 '{keywords_file_path}'에 저장했습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
